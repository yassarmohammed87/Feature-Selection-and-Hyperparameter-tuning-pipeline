{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import  SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "End to End Machine Learning Pipeline with hyperparameter tuning and Feature Selection\n",
    "\n",
    "'''\n",
    "\n",
    "class Pipeline(BaseEstimator,TransformerMixin):\n",
    "    \n",
    "    '''\n",
    "    Function Name:__init__\n",
    "\n",
    "    Parameters\n",
    "    1)label_name: str\n",
    "    Name of the target_variable\n",
    "\n",
    "    2)algo:str\n",
    "    Algorithm to use to train models.(Current available options 1.Random Forest;2.Logistic Regression)\n",
    "    3)scoring:str \n",
    "    metric to maximize\n",
    "\n",
    "    4)categorical_cols:List(str) default:[]\n",
    "    List of Columns to convert to Ordinal values\n",
    "\n",
    "    5)want_preprocess:bool default:True\n",
    "    Whether to avail preprocessing provided by the pipeline\n",
    "\n",
    "    ##########################################################################################################################\n",
    "    6)first iter:bool default:False\n",
    "    Train base models \n",
    "    Note:Only set true while running code for first time or using a previously unused algorithm(set by algo parameter) \n",
    "    for the first time.\n",
    "\n",
    "    7)want_train:bool default:False\n",
    "    train models on transformed dataset\n",
    "    Note:Pipeline return estimator(model) when this parameter is set to True\n",
    "\n",
    "    ##########################################################################################################################\n",
    "    8)recheck:bool default:False\n",
    "    recompute best features\n",
    "    '''    \n",
    "\n",
    "    def __init__(self,label_name,algo='Random_Forest',scoring='roc_auc',categorical_cols=[],want_preprocess=True,first_iter=False,want_train=False,recheck=False):\n",
    "        \n",
    "        self.label_name=label_name\n",
    "        \n",
    "        self.model_name=algo\n",
    "        self.scoring=scoring\n",
    "        self.categorical_cols=categorical_cols\n",
    "        self.want_preprocess=want_preprocess\n",
    "        self.first_iter=first_iter\n",
    "        self.want_train=want_train\n",
    "        self.recheck=recheck\n",
    "        self.best_estimator=None\n",
    "        \n",
    "    '''\n",
    "     Function Name:categorical_column dealing\n",
    "\n",
    "     Parameter\n",
    "\n",
    "     1)catergorical_columns:List(str)\n",
    "     List of columns to convert to Ordinal Values\n",
    "\n",
    "     returns Pandas Dataframe\n",
    "    '''       \n",
    "        \n",
    "        \n",
    "        \n",
    "    def categorical_column_dealing(self,categorical_columns=[]):\n",
    "        self.categorical_columns=categorical_columns\n",
    "        all_category=self.df.dtypes[self.df.dtypes==object].index\n",
    "        unwanted_category=all_category[~(all_category.isin(categorical_columns))]\n",
    "        self.df=self.df.drop(columns=unwanted_category)\n",
    "        self.reqd_features=self.df.columns\n",
    "        if len(self.categorical_cols)!=0:\n",
    "            col_transformer=ColumnTransformer([\n",
    "                (\"ord\",OrdinalEncoder(),categorical_columns),\n",
    "                    ])\n",
    "            self.encoder=col_transformer.fit_transform(self.df)\n",
    "            self.df[categorical_columns]=self.encoder\n",
    "        return self.df\n",
    "    \n",
    "    '''\n",
    "     Function Name:preprocess\n",
    "\n",
    "     Parameter\n",
    "\n",
    "     1)dataframe:dataframe default:Empty Dataframe\n",
    "     Dataframe to be processed\n",
    "\n",
    "     2)test:bool default False\n",
    "     Whether the dataset being passed is testset or not\n",
    "\n",
    "    #########################################################\n",
    "     3)want_sample bool default False\n",
    "      Balance dataset based on target variable\n",
    "      Note:IF want_sample is set to true then test should be false(default)\n",
    "    ###########################################################\n",
    "\n",
    "     returns Pandas Dataframe,Pandas Series i.e: Dataset,label\n",
    "    '''       \n",
    "    \n",
    "    def preprocess(self,dataframe=pd.DataFrame(),test=False,want_sample=False):\n",
    "        if dataframe is None:\n",
    "            self.df=dataframe\n",
    "        self.df=self.df.drop(columns=self.df.filter(like='Unnamed',axis=1).columns)\n",
    "        \n",
    "        inds=['index','id','ID','Index','INDEX']\n",
    "        print(self.df.columns)\n",
    "        try:\n",
    "            id_colname=inds[np.where(np.isin(['index','id','ID','Index','INDEX'],self.df.columns))[0][0]]\n",
    "\n",
    "            if id_colname:\n",
    "                self.df=self.df.drop(columns=[id_colname])\n",
    "            df_cols=self.df.columns\n",
    "        except:\n",
    "            df_cols=self.df.columns\n",
    "        \n",
    "        feature_df=self.df[self.df.columns.drop([self.label_name])]\n",
    "        self.reqd_features=feature_df.columns\n",
    "        \n",
    "        pipe=sklearn.pipeline.Pipeline([('imputer',SimpleImputer(strategy='median')),('scaler',StandardScaler())])\n",
    "        arr=pipe.fit_transform(feature_df)\n",
    "        \n",
    "        self.df=np.concatenate([arr,np.array(self.df[self.label_name].values).reshape(arr.shape[0],1)],axis=1)\n",
    "        self.reqd_features=np.append(np.array(feature_df.columns),self.label_name)\n",
    "        self.df=pd.DataFrame(self.df,columns=self.reqd_features)\n",
    "       \n",
    "        \n",
    "            \n",
    "\n",
    "        if test==False and want_sample==True:\n",
    "            zero_df=self.df.loc[self.df[self.label_name]==0]\n",
    "            one_df=self.df.loc[self.df[self.label_name]==1]\n",
    "            one_df=one_df.sample(np.int64(np.floor(zero_df.shape[0]/2.0)),replace=True,random_state=42)\n",
    "            self.df=pd.concat([zero_df,one_df])\n",
    "            self.df=self.df.sample(frac=1,random_state=42)\n",
    "            \n",
    "        if dataframe.empty:\n",
    "            self.df,testset=train_test_split(self.df,test_size=0.10,random_state=42)\n",
    "\n",
    "        if dataframe.empty == False:\n",
    "            return self.df[feature_df.columns],self.df[self.label_name]\n",
    "        else:\n",
    "            return self.df[feature_df.columns],self.df[self.label_name],testset[feature_df.columns],testset[self.label_name]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "     Function Name:train_models\n",
    "\n",
    "     Parameter\n",
    "\n",
    "     1)traindata:Dataframe\n",
    "     TrainSet\n",
    "\n",
    "     2)trainlabel:Series or array\n",
    "     Labels\n",
    "\n",
    "     #######################################################################################################\n",
    "     Note:i)This function trains models and stores them in your current directory\n",
    "     #####################################################################################################\n",
    "\n",
    "     returns None\n",
    "    '''       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_models(self,traindata,trainlabel):\n",
    "        \n",
    "        if self.model_name in ['logistic_regression','Logistic_regression','logistic regression','Logistic_Regression','Logistic Regression']:\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            logreg=LogisticRegression(penalty='l2',solver='liblinear',max_iter=200,random_state=42)\n",
    "            parameters=[{'C':[0.001,0.01,0.1,1,10]}]\n",
    "            grlogreg=GridSearchCV(logreg,parameters,cv=3,n_jobs=-1,scoring=self.scoring)\n",
    "            grlogreg.fit(traindata,trainlabel)\n",
    "            logreg=grlogreg.best_estimator_\n",
    "            logreg.fit(traindata,trainlabel)\n",
    "            joblib.dump(logreg,self.model_name+'_'+self.scoring + '_' + self.label_name)\n",
    "            \n",
    "        elif self.model_name in ['Random Forest','Random_Forest','Random_forest','random_forest','random forest']:       \n",
    "            n_estimators=[int(x) for x in np.linspace(start=200,stop=2000,num=10)]\n",
    "            max_features=['auto','sqrt']\n",
    "            max_depth=[int(x) for x in np.linspace(10,110,num=11)]\n",
    "            max_depth.append(None)\n",
    "            min_samples_split=[2,5,10]\n",
    "            min_samples_leaf=[1,2,4]\n",
    "            bootstrap=[True,False]\n",
    "            param_grid={'n_estimators':n_estimators,'max_features':max_features,'max_depth':max_depth,'min_samples_split':min_samples_split\n",
    "                           ,'min_samples_leaf':min_samples_leaf,'bootstrap':bootstrap}\n",
    "            rf=RandomForestClassifier()\n",
    "            rrf=RandomizedSearchCV(estimator=rf,param_distributions=param_grid,cv=3,scoring=self.scoring,verbose=2,n_iter=50,random_state=42,n_jobs=-1,refit=True)\n",
    "            rrf.fit(traindata,trainlabel)\n",
    "            joblib.dump(rrf.best_estimator_,self.model_name+ '_' +self.scoring + '_' + self.label_name)\n",
    "            \n",
    "        elif self.model_name in ['SVC','LinearSVC','svc']:\n",
    "            param_grid = {'C': [0.1, 1, 10, 100, 1000]}\n",
    "            svm=SVC(kernel='linear')\n",
    "            svsearch=GridSearchCV(svm,param_grid,cv=3,n_jobs=-1,scoring=self.scoring)\n",
    "            svsearch.fit(traindata,trainlabel)\n",
    "            joblib.dump(svsearch.best_estimator_,self.model_name+ '_' +self.scoring + '_' + self.label_name)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    '''\n",
    "     Function Name:features_selection\n",
    "\n",
    "     Parameter\n",
    "\n",
    "     1)trainset:Dataframe\n",
    "     TrainSet\n",
    "\n",
    "     2)trainlabel:Series or array\n",
    "     Labels\n",
    "\n",
    "     3)refit:bool default:False\n",
    "       Set to true if want to rerun base models to extract features\n",
    "\n",
    "     #######################################################################################################\n",
    "     Note:i)This function trains models and stores them in your current directory(doesnt store if refit=False)\n",
    "     #####################################################################################################\n",
    "\n",
    "     returns \n",
    "     1)statsdf:Dataframe\n",
    "     Stastical Data of all columns\n",
    "\n",
    "     2)self.features:estimator\n",
    "     ###################################################################################################\n",
    "     Variable List:\n",
    "     self.features.k_scores_:Returns score of model trained on Best feature subset\n",
    "     self.features.k_feature_names:Return best tuple of best features\n",
    "     self.features.subsets_:Returns a detailed dictionary holding data of all feature subsets\n",
    "     ###################################################################################################\n",
    "     3)final_ranks:List\n",
    "     features subset filtered out by statistical methods\n",
    "\n",
    "    '''\n",
    "        \n",
    "    def features_selection(self,trainset,trainlabel,refit=False):\n",
    "        info_gain=mutual_info_classif(trainset,trainlabel)\n",
    "        impo=pd.Series(info_gain,trainset.columns)\n",
    "        impo.sort_values(ascending=False,inplace=True)\n",
    "        trainset.insert(trainset.shape[1],self.label_name,trainlabel,True)\n",
    "        corr=trainset.corr()\n",
    "        cor_mat_ranks=np.abs(corr.loc[self.label_name].drop(index=self.label_name).sort_values(ascending=False))\n",
    "        trainset.drop(columns=[self.label_name],inplace=True)\n",
    "        variance_rank=np.var(trainset,axis=0).sort_values(axis=0,ascending=False)\n",
    "        \n",
    "        statsdf=pd.DataFrame([impo,cor_mat_ranks,variance_rank]).T\n",
    "        statsdf.rename(columns={'Unnamed 0':'info gain',self.label_name:'Corelation','Unnamed 1':'Variance'},inplace=True)\n",
    "        \n",
    "        final_ranks=(0.01*cor_mat_ranks+0.01*impo+0.01*variance_rank)\n",
    "        statsdf['Total']=final_ranks\n",
    "        \n",
    "        mean=np.mean(statsdf['Total'])\n",
    "        final_ranks=statsdf['Total']\n",
    "        for i in dict(final_ranks):\n",
    "            if final_ranks[i]< mean:\n",
    "                final_ranks.drop(index=[i],inplace=True)\n",
    "        self.statsdf=statsdf\n",
    "        trainset=trainset[final_ranks.index]\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        if refit==True:\n",
    "            model=joblib.load(self.model_name+'_'+self.scoring+ '_'+self.label_name)\n",
    "            feature_selector=RFECV(model, step=15, cv=2,\n",
    "            scoring=self.scoring,\n",
    "            min_features_to_select=1,n_jobs= -1)\n",
    "            n_selector=feature_selector.fit(trainset,trainlabel)\n",
    "            trainset=trainset.drop(columns=trainset.columns[~n_selector.support_])\n",
    "            \n",
    "            \n",
    "            feature_selector=SFS(model,k_features=(1,trainset.shape[1]),cv=2,forward=True,scoring=self.scoring,\n",
    "                                 n_jobs=-1)\n",
    "            features=feature_selector.fit(trainset,trainlabel,custom_feature_names=trainset.columns)\n",
    "            joblib.dump(features,'exhaustive_'+self.model_name+'_'+self.scoring+'_'+self.label_name +'_results')\n",
    "            self.best_features_=features.k_feature_names_\n",
    "            self.features=features\n",
    "        else:\n",
    "            exhaust=joblib.load('exhaustive_'+self.model_name+'_'+self.scoring+'_'+self.label_name +'_results')\n",
    "            self.features=exhaust\n",
    " ##########################################################################################################################           #\n",
    "        self.final_ranks=final_ranks\n",
    "        return self.statsdf,self.features,final_ranks\n",
    "    '''\n",
    "     Function Name:train_model\n",
    "\n",
    "     Parameter\n",
    "\n",
    "     1)trainset:Dataframe\n",
    "     TrainSet\n",
    "\n",
    "     2)trainlabel:Series or array\n",
    "     Labels\n",
    "\n",
    "     Detail:Trains model on best feature subset\n",
    "\n",
    "      #######################################################################################################\n",
    "     Note:i)This function trains models and stores them in your current directory\n",
    "\n",
    "     Can access the estimator trained on best features through the varialbe self.best_estimator\n",
    "     #####################################################################################################\n",
    "    '''   \n",
    "    \n",
    "    def train_model(self,trainset,trainlabel):\n",
    "        \n",
    "        best_features=self.features.k_feature_names_\n",
    "        if self.model_name in ['logistic_regression','Logistic_regression','logistic regression','Logistic_Regression','Logistic Regression']:\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            logreg=LogisticRegression(penalty='l2',solver='liblinear',max_iter=200,random_state=42)\n",
    "            parameters=[{'C':[0.001,0.01,0.1,1,10]}]\n",
    "            grlogreg=GridSearchCV(logreg,parameters,cv=3,n_jobs=-1,scoring=self.scoring)\n",
    "            grlogreg.fit(trainset,trainlabel)\n",
    "            logreg=grlogreg.best_estimator_\n",
    "            logreg.fit(trainset[list(best_features)],trainlabel)\n",
    "            joblib.dump(logreg,'final'+self.model_name+'_'+self.scoring + '_' + self.label_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "        elif self.model_name in ['Random Forest','Random_Forest','Random_forest','random_forest','random forest']:       \n",
    "            n_estimators=[int(x) for x in np.linspace(start=200,stop=2000,num=10)]\n",
    "            max_features=['auto','sqrt']\n",
    "            max_depth=[int(x) for x in np.linspace(10,110,num=11)]\n",
    "            max_depth.append(None)\n",
    "            min_samples_split=[2,5,10]\n",
    "            min_samples_leaf=[1,2,4]\n",
    "            bootstrap=[True,False]\n",
    "            param_grid={'n_estimators':n_estimators,'max_features':max_features,'max_depth':max_depth,'min_samples_split':min_samples_split\n",
    "                           ,'min_samples_leaf':min_samples_leaf,'bootstrap':bootstrap}\n",
    "            rf=RandomForestClassifier()\n",
    "            rrf=RandomizedSearchCV(estimator=rf,param_distributions=param_grid,cv=3,scoring=self.scoring,verbose=2,n_iter=40,random_state=42,n_jobs=-1,refit=True)\n",
    "            rrf.fit(trainset[list(best_features)],trainlabel)\n",
    "            joblib.dump(rrf.best_estimator_,'final'+self.model_name+ '_' +self.scoring + '_' + self.label_name)\n",
    "            \n",
    "        elif self.model_name in ['SVC','LinearSVC','svc']:\n",
    "            param_grid = {'C': [0.1, 1, 10, 100, 1000]}\n",
    "            svm=SVC(kernel='linear')\n",
    "            svsearch=GridSearchCV(svm,param_grid,cv=3,n_jobs=-1,scoring=self.scoring)\n",
    "            svsearch.fit(trainset,trainlabel)\n",
    "            joblib.dump(svsearch.best_estimator_,'final'+self.model_name+ '_' +self.scoring + '_' + self.label_name)\n",
    "    \n",
    "        self.best_estimator=joblib.load('final'+self.model_name+ '_' +self.scoring + '_' + self.label_name)\n",
    "        \n",
    "        return self.best_estimator\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Function Name:predict_and_plot\n",
    "\n",
    "     Parameter\n",
    "\n",
    "     1)trainset:Dataframe\n",
    "     TrainSet\n",
    "\n",
    "     2)trainlabel:Series or array\n",
    "     Labels\n",
    "\n",
    "     3)option:string\n",
    "     #############################################################\n",
    "     option list\n",
    "     1)predictions\n",
    "     2)'accuracy_score'\n",
    "     3)precision_score\n",
    "     4)recall_score\n",
    "     5)roc_auc_score\n",
    "     6)plot_roc_curve\n",
    "     7)confusion_matrix\n",
    "\n",
    "     4)estimator:sklearn estimator\n",
    "     model trained on dataset containing best features subset provided by Features_selection method\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def predict_and_plot(self,option,trainset,trainlabel,estimator=None):\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        y=estimator.predict(trainset)\n",
    "        if option=='predictions':\n",
    "            return y\n",
    "        \n",
    "        if option=='accuracy_score':\n",
    "            return accuracy_score(trainlabel,y)\n",
    "        \n",
    "        if option=='precision_score':\n",
    "            return sklearn.metrics.precision_score(trainlabel,y)\n",
    "        if option=='recall_score':\n",
    "            return sklearn.metrics.recall_score(trainlabel,y)\n",
    "        if option=='roc_auc_score':\n",
    "            return sklearn.metrics.roc_auc_score(trainlabel,y)\n",
    "        if option=='plot_roc_curve':\n",
    "            sklearn.metrics.plot_roc_curve(estimator,trainset,trainlabel)\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "        if option=='confusion_matrix':\n",
    "            plot_confusion_matrix(estimator,trainset,trainlabel)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self,trainset,trainlabel=None):\n",
    "        return self\n",
    "\n",
    "    '''\n",
    "    Function Name:transform\n",
    "\n",
    "    Parameter\n",
    "\n",
    "    1)path:str or os.path\n",
    "    path to csv file which contains the data\n",
    "\n",
    "    Detail:Applies all above mentioned processing to the dataset based on the parameters provided to __init__ function.\n",
    "\n",
    "    returns best_features.k_feature_name:list of names of best feature subset \n",
    "    '''\n",
    "    def transform(self,path):\n",
    "        self.df=pd.read_csv(path)\n",
    "        self.reqd_features=self.df.columns.drop([self.label_name])\n",
    "        \n",
    "        self.transform_df=self.categorical_column_dealing(self.categorical_cols)\n",
    "        if self.want_preprocess ==True:\n",
    "            self.transform_df,self.transform_label=self.preprocess(dataframe=self.transform_df,want_sample=False)\n",
    "        \n",
    "        if self.first_iter==True:\n",
    "            self.train_models(self.transform_df,self.transform_label)\n",
    "        stats,best_features,filtered_features=self.features_selection(self.transform_df,self.transform_label,self.recheck)\n",
    "        if self.want_train==True:\n",
    "            self.best_estimator=self.train_model(self.transform_df[list(best_features.k_feature_names_)],self.transform_label)\n",
    "        else:\n",
    "            self.best_estimator=joblib.load('final'+self.model_name+ '_' +self.scoring + '_' + self.label_name)\n",
    "        \n",
    "        return (self.transform_df[list(best_features.k_feature_names_)],self.transform_label)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
